{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4U9yj8xgeNA+OQjec6233",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravrk215/anime/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        },
        "id": "BPB1ewCZ1-AQ",
        "outputId": "28b21f56-890a-431c-ef89-dc082a6590ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Using device: cpu\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://89f3596b919b11b48f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89f3596b919b11b48f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Started processing...\n",
            "üîç Extracting frames...\n",
            "üñºÔ∏è Total frames extracted: 91\n",
            "üìê Shape before squeeze: torch.Size([1, 3, 91, 480, 584])\n",
            "üìê Shape after squeeze: torch.Size([3, 91, 480, 584])\n",
            "‚úÖ uniform_temporal_subsample done. Shape: torch.Size([3, 36, 480, 584])\n",
            "‚úÖ Resizing done. Shape: torch.Size([3, 36, 512, 622])\n",
            "‚úÖ Center crop done. Shape: torch.Size([3, 36, 512, 512])\n",
            "üé¨ Model output ready. Writing video without audio...\n",
            "üé® Model output pixel range: 127 to 197\n",
            "‚úÖ Done! Final video (no audio) saved as: anime_output_video_only.mp4\n"
          ]
        }
      ],
      "source": [
        "# üîß Install dependencies\n",
        "!pip install moviepy gradio av torch torchvision --quiet\n",
        "\n",
        "# ‚úÖ Imports\n",
        "import torch, gc, math, os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, center_crop\n",
        "from torchvision.io import write_video\n",
        "import moviepy.editor as mp\n",
        "import av\n",
        "import moviepy.editor as mp\n",
        "import gradio as gr\n",
        "\n",
        "# ‚úÖ Dummy model (replace with AnimeGAN2 model if available)\n",
        "class DummyAnimeModel(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Simulate 'style': invert and darken\n",
        "        styled = 1 - x       # invert color\n",
        "        styled = styled * 0.5  # darken\n",
        "        return styled\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DummyAnimeModel().to(device)\n",
        "print(f\"üß† Using device: {device}\")\n",
        "\n",
        "# ‚úÖ Frame sampling function (instead of pytorchvideo)\n",
        "def uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = 1) -> torch.Tensor:\n",
        "    t = x.shape[temporal_dim]\n",
        "    indices = torch.linspace(0, t - 1, num_samples).clamp(0, t - 1).long()\n",
        "    return x.index_select(temporal_dim, indices)\n",
        "\n",
        "# ‚úÖ Resize keeping aspect ratio\n",
        "def predict_fn(filepath, start_sec, duration, out_fps):\n",
        "    try:\n",
        "        print(\"üöÄ Started processing...\")\n",
        "\n",
        "        container = av.open(filepath)\n",
        "        container.seek(int(start_sec * av.time_base))\n",
        "\n",
        "        frames = []\n",
        "        print(\"üîç Extracting frames...\")\n",
        "        for frame in container.decode(video=0):\n",
        "            if frame.time < start_sec:\n",
        "                continue\n",
        "            if frame.time > start_sec + duration:\n",
        "                break\n",
        "            img = frame.to_image()\n",
        "            tensor = to_tensor(img).unsqueeze(0)\n",
        "            frames.append(tensor)\n",
        "\n",
        "        print(f\"üñºÔ∏è Total frames extracted: {len(frames)}\")\n",
        "        if len(frames) == 0:\n",
        "            raise ValueError(\"‚ùå No frames found in the selected duration.\")\n",
        "\n",
        "        video_tensor = torch.cat(frames, dim=0).permute(1, 0, 2, 3).unsqueeze(0)\n",
        "\n",
        "        print(\"üìê Shape before squeeze:\", video_tensor.shape)\n",
        "        if video_tensor.dim() == 5:\n",
        "            video_tensor = video_tensor.squeeze(0)\n",
        "        print(\"üìê Shape after squeeze:\", video_tensor.shape)\n",
        "\n",
        "        x = uniform_temporal_subsample(video_tensor, duration * out_fps)\n",
        "        print(\"‚úÖ uniform_temporal_subsample done. Shape:\", x.shape)\n",
        "\n",
        "        x = short_side_scale(x, 512)\n",
        "        print(\"‚úÖ Resizing done. Shape:\", x.shape)\n",
        "\n",
        "        x = center_crop(x, [512, 512])\n",
        "        print(\"‚úÖ Center crop done. Shape:\", x.shape)\n",
        "\n",
        "        x = x / 255.0\n",
        "        x = x.permute(1, 0, 2, 3)  # [T, C, H, W]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(x.to(device)).cpu()\n",
        "            out = (out * 0.5 + 0.5).clamp(0, 1) * 255.\n",
        "            output_np = out.permute(0, 2, 3, 1).byte().numpy()\n",
        "\n",
        "        print(\"üé¨ Model output ready. Writing video without audio...\")\n",
        "\n",
        "        out_file = \"anime_output_video_only.mp4\"\n",
        "        print(\"üé® Model output pixel range:\", output_np.min(), \"to\", output_np.max())\n",
        "\n",
        "        write_video(\n",
        "            out_file,\n",
        "            torch.from_numpy(output_np),\n",
        "            fps=out_fps\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Done! Final video (no audio) saved as:\", out_file)\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error occurred:\", str(e))\n",
        "        return \"Error: \" + str(e)\n",
        "\n",
        "# ‚úÖ Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üé¨ Anime Video Converter (with Audio)\")\n",
        "    video_input = gr.Video(label=\"Upload your .mp4 video\")\n",
        "    start = gr.Slider(0, 300, step=1, value=0, label=\"Start Time (sec)\")\n",
        "    duration = gr.Slider(1, 10, step=1, value=3, label=\"Duration (sec)\")\n",
        "    fps = gr.Slider(6, 30, step=6, value=12, label=\"Output FPS\")\n",
        "    convert = gr.Button(\"üé® Convert to Anime\")\n",
        "    output = gr.Video(label=\"Anime Output\")\n",
        "\n",
        "    convert.click(fn=predict_fn, inputs=[video_input, start, duration, fps], outputs=output)\n",
        "\n",
        "demo.launch(debug=True,share=True)\n"
      ]
    }
  ]
}