{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4U9yj8xgeNA+OQjec6233",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravrk215/anime/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPB1ewCZ1-AQ",
        "outputId": "77e8d016-78c7-45a9-c6b2-fa984640a7ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m147.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# üîß Install dependencies\n",
        "!pip install moviepy gradio av torch torchvision --quiet\n",
        "\n",
        "# ‚úÖ Imports\n",
        "import torch, gc, math, os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor, center_crop\n",
        "from torchvision.io import write_video\n",
        "import moviepy.editor as mp\n",
        "import av\n",
        "import moviepy.editor as mp\n",
        "import gradio as gr\n",
        "\n",
        "# ‚úÖ Dummy model (replace with AnimeGAN2 model if available)\n",
        "class DummyAnimeModel(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        # Simulate 'style': invert and darken\n",
        "        styled = 1 - x       # invert color\n",
        "        styled = styled * 0.5  # darken\n",
        "        return styled\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DummyAnimeModel().to(device)\n",
        "print(f\"üß† Using device: {device}\")\n",
        "\n",
        "# ‚úÖ Frame sampling function (instead of pytorchvideo)\n",
        "def uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = 1) -> torch.Tensor:\n",
        "    t = x.shape[temporal_dim]\n",
        "    indices = torch.linspace(0, t - 1, num_samples).clamp(0, t - 1).long()\n",
        "    return x.index_select(temporal_dim, indices)\n",
        "\n",
        "# ‚úÖ Resize keeping aspect ratio\n",
        "def predict_fn(filepath, start_sec, duration, out_fps):\n",
        "    try:\n",
        "        print(\"üöÄ Started processing...\")\n",
        "\n",
        "        container = av.open(filepath)\n",
        "        container.seek(int(start_sec * av.time_base))\n",
        "\n",
        "        frames = []\n",
        "        print(\"üîç Extracting frames...\")\n",
        "        for frame in container.decode(video=0):\n",
        "            if frame.time < start_sec:\n",
        "                continue\n",
        "            if frame.time > start_sec + duration:\n",
        "                break\n",
        "            img = frame.to_image()\n",
        "            tensor = to_tensor(img).unsqueeze(0)\n",
        "            frames.append(tensor)\n",
        "\n",
        "        print(f\"üñºÔ∏è Total frames extracted: {len(frames)}\")\n",
        "        if len(frames) == 0:\n",
        "            raise ValueError(\"‚ùå No frames found in the selected duration.\")\n",
        "\n",
        "        video_tensor = torch.cat(frames, dim=0).permute(1, 0, 2, 3).unsqueeze(0)\n",
        "\n",
        "        print(\"üìê Shape before squeeze:\", video_tensor.shape)\n",
        "        if video_tensor.dim() == 5:\n",
        "            video_tensor = video_tensor.squeeze(0)\n",
        "        print(\"üìê Shape after squeeze:\", video_tensor.shape)\n",
        "\n",
        "        x = uniform_temporal_subsample(video_tensor, duration * out_fps)\n",
        "        print(\"‚úÖ uniform_temporal_subsample done. Shape:\", x.shape)\n",
        "\n",
        "        x = short_side_scale(x, 512)\n",
        "        print(\"‚úÖ Resizing done. Shape:\", x.shape)\n",
        "\n",
        "        x = center_crop(x, [512, 512])\n",
        "        print(\"‚úÖ Center crop done. Shape:\", x.shape)\n",
        "\n",
        "        x = x / 255.0\n",
        "        x = x.permute(1, 0, 2, 3)  # [T, C, H, W]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(x.to(device)).cpu()\n",
        "            out = (out * 0.5 + 0.5).clamp(0, 1) * 255.\n",
        "            output_np = out.permute(0, 2, 3, 1).byte().numpy()\n",
        "\n",
        "        print(\"üé¨ Model output ready. Writing video without audio...\")\n",
        "\n",
        "        out_file = \"anime_output_video_only.mp4\"\n",
        "        print(\"üé® Model output pixel range:\", output_np.min(), \"to\", output_np.max())\n",
        "\n",
        "        write_video(\n",
        "            out_file,\n",
        "            torch.from_numpy(output_np),\n",
        "            fps=out_fps\n",
        "        )\n",
        "\n",
        "        print(\"‚úÖ Done! Final video (no audio) saved as:\", out_file)\n",
        "        return out_file\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error occurred:\", str(e))\n",
        "        return \"Error: \" + str(e)\n",
        "\n",
        "# ‚úÖ Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üé¨ Anime Video Converter (with Audio)\")\n",
        "    video_input = gr.Video(label=\"Upload your .mp4 video\")\n",
        "    start = gr.Slider(0, 300, step=1, value=0, label=\"Start Time (sec)\")\n",
        "    duration = gr.Slider(1, 10, step=1, value=3, label=\"Duration (sec)\")\n",
        "    fps = gr.Slider(6, 30, step=6, value=12, label=\"Output FPS\")\n",
        "    convert = gr.Button(\"üé® Convert to Anime\")\n",
        "    output = gr.Video(label=\"Anime Output\")\n",
        "\n",
        "    convert.click(fn=predict_fn, inputs=[video_input, start, duration, fps], outputs=output)\n",
        "\n",
        "demo.launch(debug=True,share=True)\n"
      ]
    }
  ]
}