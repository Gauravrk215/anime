{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDEFC7jQulH8x9Rm1FEKfd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravrk215/anime/blob/main/Untitled1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPB1ewCZ1-AQ",
        "outputId": "0e730ff8-f0fd-4fdd-f31a-1dd35efd117b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ],
      "source": [
        "# üîß Install dependencies\n",
        "!pip install moviepy gradio av torch torchvision --quiet\n",
        "\n",
        "# ‚úÖ Imports\n",
        "import torch, gc, math, os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision.transforms.functional import to_tensor\n",
        "from torchvision.io import write_video\n",
        "import av\n",
        "import moviepy.editor as mp\n",
        "import gradio as gr\n",
        "\n",
        "# ‚úÖ Dummy model (replace with AnimeGAN2 model if available)\n",
        "class DummyAnimeModel(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x  # No-op for testing\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DummyAnimeModel().to(device)\n",
        "print(f\"üß† Using device: {device}\")\n",
        "\n",
        "# ‚úÖ Frame sampling function (instead of pytorchvideo)\n",
        "def uniform_temporal_subsample(x: torch.Tensor, num_samples: int, temporal_dim: int = 1) -> torch.Tensor:\n",
        "    t = x.shape[temporal_dim]\n",
        "    indices = torch.linspace(0, t - 1, num_samples).clamp(0, t - 1).long()\n",
        "    return x.index_select(temporal_dim, indices)\n",
        "\n",
        "# ‚úÖ Resize keeping aspect ratio\n",
        "def short_side_scale(x: torch.Tensor, size: int) -> torch.Tensor:\n",
        "    if x.dim() == 5:\n",
        "        x = x.squeeze(0)\n",
        "    if x.dim() != 4:\n",
        "        raise ValueError(f\"Expected 4D tensor but got shape {x.shape}\")\n",
        "    c, t, h, w = x.shape\n",
        "    if w < h:\n",
        "        new_h = int((h / w) * size)\n",
        "        new_w = size\n",
        "    else:\n",
        "        new_w = int((w / h) * size)\n",
        "        new_h = size\n",
        "    return torch.nn.functional.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=False)\n",
        "\n",
        "# ‚úÖ Main processor\n",
        "def predict_fn(filepath, start_sec, duration, out_fps):\n",
        "    print(\"üöÄ Started processing...\")\n",
        "    container = av.open(filepath)\n",
        "    video_stream = container.streams.video[0]\n",
        "    container.seek(int(start_sec * av.time_base))\n",
        "\n",
        "    frames = []\n",
        "    audio_clip = mp.VideoFileClip(filepath).audio.subclip(start_sec, start_sec + duration)\n",
        "\n",
        "    for frame in container.decode(video=0):\n",
        "        if frame.time < start_sec:\n",
        "            continue\n",
        "        if frame.time > start_sec + duration:\n",
        "            break\n",
        "        img = frame.to_image()\n",
        "        tensor = to_tensor(img).unsqueeze(0)\n",
        "        frames.append(tensor)\n",
        "\n",
        "    if len(frames) == 0:\n",
        "        raise ValueError(\"‚ùå No frames found.\")\n",
        "\n",
        "    video_tensor = torch.cat(frames, dim=0).permute(1, 0, 2, 3).unsqueeze(0)\n",
        "\n",
        "    if video_tensor.dim() == 5:\n",
        "        video_tensor = video_tensor.squeeze(0)\n",
        "\n",
        "    x = uniform_temporal_subsample(video_tensor, duration * out_fps)\n",
        "    x = short_side_scale(x, 512)\n",
        "    x = torch.nn.functional.center_crop(x, [512, 512])\n",
        "    x = x / 255.0\n",
        "    x = x.permute(1, 0, 2, 3)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model(x.to(device)).cpu()\n",
        "        out = (out * 0.5 + 0.5).clamp(0, 1) * 255.\n",
        "        output_np = out.permute(0, 2, 3, 1).byte().numpy()\n",
        "\n",
        "    out_file = \"anime_output_with_audio.mp4\"\n",
        "    write_video(\n",
        "        out_file,\n",
        "        torch.from_numpy(output_np),\n",
        "        fps=out_fps,\n",
        "        audio_array=audio_clip.to_soundarray(fps=44100).T,\n",
        "        audio_fps=44100,\n",
        "        audio_codec='aac'\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Done. Returning final video.\")\n",
        "    return out_file\n",
        "\n",
        "# ‚úÖ Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## üé¨ Anime Video Converter (with Audio)\")\n",
        "    video_input = gr.Video(label=\"Upload your .mp4 video\")\n",
        "    start = gr.Slider(0, 300, step=1, value=0, label=\"Start Time (sec)\")\n",
        "    duration = gr.Slider(1, 10, step=1, value=3, label=\"Duration (sec)\")\n",
        "    fps = gr.Slider(6, 30, step=6, value=12, label=\"Output FPS\")\n",
        "    convert = gr.Button(\"üé® Convert to Anime\")\n",
        "    output = gr.Video(label=\"Anime Output\")\n",
        "\n",
        "    convert.click(fn=predict_fn, inputs=[video_input, start, duration, fps], outputs=output)\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ]
}